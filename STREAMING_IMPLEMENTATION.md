# æµå¼ä¼ è¾“åŠŸèƒ½å®ç°æ€»ç»“

## å®ç°æ¦‚è¿°

å·²æˆåŠŸå®ç°ä»»åŠ¡ 9.3ï¼šæµå¼ä¼ è¾“ï¼ˆStreamingï¼‰åŠŸèƒ½æ”¯æŒã€‚è¯¥åŠŸèƒ½å…è®¸å®¢æˆ·ç«¯å®æ—¶æ¥æ”¶ LLM å“åº”ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

## å®ç°çš„åŠŸèƒ½

### 1. æ ¸å¿ƒåŠŸèƒ½

âœ… **Server-Sent Events (SSE) æ”¯æŒ**
- æ ‡å‡† SSE æ ¼å¼è¾“å‡º
- å…¼å®¹ OpenAI æµå¼ API è§„èŒƒ
- æ­£ç¡®çš„ `data:` å‰ç¼€å’Œ `[DONE]` ç»“æŸæ ‡è®°

âœ… **æµå¼è¯·æ±‚å¤„ç†**
- æ£€æµ‹ `stream=true` å‚æ•°
- è‡ªåŠ¨è·¯ç”±åˆ°æµå¼å¤„ç†é€»è¾‘
- æ”¯æŒæ‰€æœ‰æ ‡å‡† OpenAI å‚æ•°

âœ… **å†…å®¹ç´¯ç§¯å’Œä¼šè¯ç®¡ç†**
- è‡ªåŠ¨ç´¯ç§¯æµå¼å†…å®¹
- æ›´æ–°ä¼šè¯å†å²
- Token ä½¿ç”¨ç»Ÿè®¡

âœ… **é”™è¯¯å¤„ç†**
- æµå¼é”™è¯¯ä»¥ SSE æ ¼å¼è¿”å›
- å®Œæ•´çš„å¼‚å¸¸æ•è·å’Œæ—¥å¿—è®°å½•
- ä¼˜é›…çš„é”™è¯¯é™çº§

### 2. ä»£ç ä¿®æ”¹

#### æ–‡ä»¶ï¼š`src/api/endpoints.py`

**æ–°å¢åŠŸèƒ½**ï¼š
- å¯¼å…¥ `StreamingResponse` å’Œ `json`
- ä¿®æ”¹ `chat_completions` ç«¯ç‚¹æ”¯æŒæµå¼å“åº”
- æ–°å¢ `_stream_chat_completion` å¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°

**å…³é”®å®ç°**ï¼š
```python
# æ£€æµ‹æµå¼è¯·æ±‚
if request.stream:
    return StreamingResponse(
        _stream_chat_completion(...),
        media_type="text/event-stream"
    )

# æµå¼ç”Ÿæˆå™¨
async def _stream_chat_completion(...):
    async for chunk in provider_mgr.stream_request(...):
        # ç´¯ç§¯å†…å®¹
        # æ ¼å¼åŒ–ä¸º SSE
        yield f"data: {chunk_json}\n\n"
    yield "data: [DONE]\n\n"
```

#### æ–‡ä»¶ï¼š`src/core/provider_manager.py`

**æ–°å¢åŠŸèƒ½**ï¼š
- å¯¼å…¥ `AsyncIterator` å’Œ `json`
- å¯¼å…¥ `ChatCompletionStreamResponse` å’Œ `StreamChoice`
- æ–°å¢ `stream_request` æ–¹æ³•

**å…³é”®å®ç°**ï¼š
```python
async def stream_request(...) -> AsyncIterator[ChatCompletionStreamResponse]:
    # ä½¿ç”¨ httpx.stream å‘èµ·æµå¼è¯·æ±‚
    async with client.stream("POST", url, ...) as response:
        # é€è¡Œè§£æ SSE
        async for line in response.aiter_lines():
            if line.startswith("data: "):
                # è§£æ JSON å—
                chunk = ChatCompletionStreamResponse(**chunk_data)
                yield chunk
```

#### æ–‡ä»¶ï¼š`src/models/openai.py`

**å·²æœ‰æ”¯æŒ**ï¼š
- `ChatCompletionStreamResponse` æ•°æ®æ¨¡å‹
- `StreamChoice` æ•°æ®æ¨¡å‹
- å®Œæ•´çš„æµå¼å“åº”ç»“æ„å®šä¹‰

### 3. æµ‹è¯•ä»£ç 

#### æ–‡ä»¶ï¼š`tests/test_streaming.py`

**æµ‹è¯•è¦†ç›–**ï¼š
- âœ… æµå¼å“åº”æ ¼å¼éªŒè¯
- âœ… å†…å®¹ç´¯ç§¯æµ‹è¯•
- âœ… SSE æ ¼å¼æµ‹è¯•
- âœ… [DONE] æ¶ˆæ¯æ ¼å¼æµ‹è¯•
- âœ… åŸºæœ¬æµå¼è¯·æ±‚æµç¨‹æµ‹è¯•
- âœ… æµå¼å‚æ•°å¤„ç†æµ‹è¯•

#### æ–‡ä»¶ï¼š`test_streaming.py`

**æ‰‹åŠ¨æµ‹è¯•è„šæœ¬**ï¼š
- æµå¼è¯·æ±‚ç¤ºä¾‹
- éæµå¼è¯·æ±‚å¯¹æ¯”
- å®Œæ•´çš„é”™è¯¯å¤„ç†

### 4. æ–‡æ¡£

#### æ–‡ä»¶ï¼š`docs/STREAMING.md`

**å®Œæ•´çš„æµå¼ä¼ è¾“æŒ‡å—**ï¼š
- åŠŸèƒ½ç‰¹æ€§è¯´æ˜
- ä½¿ç”¨æ–¹æ³•å’Œç¤ºä¾‹ï¼ˆPythonã€JavaScriptã€cURLï¼‰
- å“åº”æ ¼å¼è¯¦è§£
- ä¸éæµå¼æ¨¡å¼å¯¹æ¯”
- å†…éƒ¨å®ç°æµç¨‹å›¾
- æ€§èƒ½è€ƒè™‘å’Œæ•…éšœæ’é™¤

#### æ–‡ä»¶ï¼š`README.md`

**æ›´æ–°å†…å®¹**ï¼š
- æ·»åŠ æµå¼ä¼ è¾“åŠŸèƒ½ç‰¹æ€§
- æ·»åŠ æµå¼è¯·æ±‚ç¤ºä¾‹
- æ·»åŠ æ–‡æ¡£é“¾æ¥

#### æ–‡ä»¶ï¼š`docs/API.md`

**æ›´æ–°å†…å®¹**ï¼š
- æ·»åŠ æµå¼å“åº”æ ¼å¼è¯´æ˜
- æ·»åŠ æµå¼è¯·æ±‚ç¤ºä¾‹
- æ·»åŠ æ–‡æ¡£äº¤å‰å¼•ç”¨

## æŠ€æœ¯ç»†èŠ‚

### SSE æ ¼å¼

```
data: <JSON>\n\n
```

æ¯ä¸ªæ¶ˆæ¯å—ï¼š
- ä»¥ `data: ` å¼€å¤´
- åè·Ÿ JSON æ ¼å¼çš„å“åº”å—
- ä»¥ä¸¤ä¸ªæ¢è¡Œç¬¦ç»“æŸ

ç»“æŸæ ‡è®°ï¼š
```
data: [DONE]\n\n
```

### å†…å®¹ç´¯ç§¯

æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­ï¼š
1. ç´¯ç§¯æ‰€æœ‰ `delta.content` å­—æ®µ
2. æµç»“æŸåæ„å»ºå®Œæ•´çš„ `Message` å¯¹è±¡
3. æ·»åŠ åˆ°ä¼šè¯å†å²
4. æ›´æ–° token ç»Ÿè®¡

### Token ä¼°ç®—

æµå¼æ¨¡å¼ä¸‹ä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ï¼š
- æç¤º tokens: `sum(len(msg.content) for msg in messages) // 4`
- å®Œæˆ tokens: `len(accumulated_content) // 4`

## å…¼å®¹æ€§

### OpenAI API å…¼å®¹æ€§

âœ… å®Œå…¨å…¼å®¹ OpenAI æµå¼ APIï¼š
- ç›¸åŒçš„è¯·æ±‚æ ¼å¼
- ç›¸åŒçš„å“åº”æ ¼å¼
- ç›¸åŒçš„ SSE åè®®

### å®¢æˆ·ç«¯æ”¯æŒ

âœ… æ”¯æŒçš„å®¢æˆ·ç«¯ï¼š
- Python (httpx, requests)
- JavaScript/TypeScript (fetch, axios)
- cURL
- OpenWebUI
- ä»»ä½•æ”¯æŒ SSE çš„ HTTP å®¢æˆ·ç«¯

## ä½¿ç”¨ç¤ºä¾‹

### Python å®¢æˆ·ç«¯

```python
import httpx
import json

async with httpx.AsyncClient() as client:
    async with client.stream(
        "POST",
        "http://localhost:8000/v1/chat/completions",
        json={
            "model": "deepseek/deepseek-chat",
            "messages": [{"role": "user", "content": "ä½ å¥½"}],
            "stream": True
        }
    ) as response:
        async for line in response.aiter_lines():
            if line.startswith("data: "):
                data = line[6:]
                if data.strip() == "[DONE]":
                    break
                chunk = json.loads(data)
                content = chunk["choices"][0]["delta"].get("content", "")
                print(content, end="", flush=True)
```

### JavaScript å®¢æˆ·ç«¯

```javascript
const response = await fetch('http://localhost:8000/v1/chat/completions', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({
    model: 'deepseek/deepseek-chat',
    messages: [{role: 'user', content: 'ä½ å¥½'}],
    stream: true
  })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const {done, value} = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  // å¤„ç† SSE æ•°æ®
}
```

## æ€§èƒ½ç‰¹ç‚¹

### ä¼˜åŠ¿

- âš¡ é™ä½é¦–å­—èŠ‚æ—¶é—´ï¼ˆTTFBï¼‰
- ğŸ‘ æ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼ˆå®æ—¶åé¦ˆï¼‰
- ğŸ“Š é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯

### æ³¨æ„äº‹é¡¹

- éœ€è¦ä¿æŒè¿æ¥ç›´åˆ°æµç»“æŸ
- Token ç»Ÿè®¡ä¸ºä¼°ç®—å€¼
- å®¢æˆ·ç«¯éœ€è¦æ­£ç¡®å¤„ç† SSE æ ¼å¼

## æµ‹è¯•éªŒè¯

### å•å…ƒæµ‹è¯•

```bash
pytest tests/test_streaming.py -v
```

### æ‰‹åŠ¨æµ‹è¯•

```bash
python test_streaming.py
```

### é›†æˆæµ‹è¯•

æµå¼ä¼ è¾“å·²é›†æˆåˆ°ç°æœ‰çš„ç«¯ç‚¹æµ‹è¯•ä¸­ã€‚

## åç»­æ”¹è¿›

å¯èƒ½çš„å¢å¼ºåŠŸèƒ½ï¼š

- [ ] æ”¯æŒå¤šä¸ªå¹¶å‘æµ
- [ ] æ›´ç²¾ç¡®çš„ token è®¡æ•°ï¼ˆä½¿ç”¨ tiktokenï¼‰
- [ ] æµå¼ä¼ è¾“æ€§èƒ½ç›‘æ§
- [ ] æ–­ç‚¹ç»­ä¼ æ”¯æŒ
- [ ] æµå¼ä¼ è¾“é€Ÿç‡é™åˆ¶

## ç›¸å…³æ–‡ä»¶

### æ ¸å¿ƒå®ç°
- `src/api/endpoints.py` - æµå¼ç«¯ç‚¹å®ç°
- `src/core/provider_manager.py` - æµå¼è¯·æ±‚ç®¡ç†
- `src/models/openai.py` - æ•°æ®æ¨¡å‹å®šä¹‰

### æµ‹è¯•
- `tests/test_streaming.py` - å•å…ƒæµ‹è¯•
- `test_streaming.py` - æ‰‹åŠ¨æµ‹è¯•è„šæœ¬

### æ–‡æ¡£
- `docs/STREAMING.md` - å®Œæ•´æŒ‡å—
- `docs/API.md` - API æ–‡æ¡£
- `README.md` - ä¸»æ–‡æ¡£

## æ€»ç»“

æµå¼ä¼ è¾“åŠŸèƒ½å·²å®Œæ•´å®ç°å¹¶é€šè¿‡æµ‹è¯•ã€‚è¯¥åŠŸèƒ½ï¼š

âœ… ç¬¦åˆ OpenAI API è§„èŒƒ
âœ… æä¾›å®Œæ•´çš„é”™è¯¯å¤„ç†
âœ… è‡ªåŠ¨ç®¡ç†ä¼šè¯å’Œ token ç»Ÿè®¡
âœ… åŒ…å«å®Œæ•´çš„æ–‡æ¡£å’Œç¤ºä¾‹
âœ… é€šè¿‡å•å…ƒæµ‹è¯•éªŒè¯

å¯ä»¥ç«‹å³æŠ•å…¥ä½¿ç”¨ï¼Œä¸ºç”¨æˆ·æä¾›å®æ—¶çš„ LLM å“åº”ä½“éªŒã€‚


## æ€è€ƒæ¨¡å‹æ”¯æŒï¼ˆæ–°å¢ï¼‰

### æ¦‚è¿°

åœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¹¶è§£å†³äº†å¯¹æ€è€ƒæ¨¡å‹ï¼ˆReasoning Modelsï¼‰çš„å…¼å®¹æ€§é—®é¢˜ã€‚è¿™äº›æ¨¡å‹ï¼ˆå¦‚ DeepSeek-R1ã€OpenAI o1ï¼‰åœ¨æµå¼è¾“å‡ºæ—¶ä¼šå…ˆè¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œç„¶åè¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

### æ”¯æŒçš„æ¨¡å‹

âœ… **DeepSeek-R1**: ä½¿ç”¨ `reasoning_content` å­—æ®µè¾“å‡ºæ€è€ƒè¿‡ç¨‹
âœ… **OpenAI o1**: ä½¿ç”¨ `thinking` å­—æ®µè¾“å‡ºæ€è€ƒè¿‡ç¨‹
âœ… **å…¶ä»–æ€è€ƒæ¨¡å‹**: è‡ªåŠ¨è¯†åˆ«å’Œå¤„ç†ä»»æ„ delta å­—æ®µ

### å®ç°ç»†èŠ‚

#### 1. Delta å­—æ®µå¤„ç†

ä¿®æ”¹äº† `_stream_chat_completion` å‡½æ•°ï¼Œæ”¯æŒå¤šç§ delta å­—æ®µï¼š

```python
# ç´¯ç§¯å¸¸è§„å†…å®¹
if delta.get("content"):
    accumulated_content += delta["content"]

# ç´¯ç§¯æ¨ç†/æ€è€ƒå†…å®¹
if delta.get("reasoning_content"):
    accumulated_reasoning += delta["reasoning_content"]
elif delta.get("thinking"):
    accumulated_reasoning += delta["thinking"]
```

#### 2. å†…å®¹è½¬å‘

æ‰€æœ‰ delta å­—æ®µéƒ½ä¼šè¢«å®Œæ•´è½¬å‘ç»™å®¢æˆ·ç«¯ï¼š

```python
# ä¸è¿‡æ»¤ä»»ä½•å­—æ®µï¼Œå®Œæ•´è½¬å‘
chunk_json = chunk.model_dump_json()
yield f"data: {chunk_json}\n\n"
```

#### 3. ä¼šè¯å†å²å­˜å‚¨

- **æœ€ç»ˆç­”æ¡ˆ**: å­˜å‚¨ `content` å­—æ®µåˆ°ä¼šè¯å†å²
- **æ€è€ƒè¿‡ç¨‹**: è®°å½•åˆ°æ—¥å¿—ï¼Œä¸å­˜å‚¨åˆ°ä¼šè¯å†å²ï¼ˆé¿å…ä¸Šä¸‹æ–‡è†¨èƒ€ï¼‰
- **ç‰¹æ®Šæƒ…å†µ**: å¦‚æœåªæœ‰æ€è€ƒå†…å®¹æ²¡æœ‰æœ€ç»ˆç­”æ¡ˆï¼Œåˆ™å­˜å‚¨æ€è€ƒå†…å®¹

```python
# ä¼˜å…ˆä½¿ç”¨æœ€ç»ˆç­”æ¡ˆï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ€è€ƒå†…å®¹
assistant_message = Message(
    role="assistant",
    content=full_content if full_content else accumulated_reasoning
)
```

#### 4. Token ç»Ÿè®¡

Token ç»Ÿè®¡åŒ…å«æ€è€ƒå†…å®¹å’Œæœ€ç»ˆç­”æ¡ˆçš„æ€»é•¿åº¦ï¼š

```python
total_content_length = len(accumulated_content) + len(accumulated_reasoning)
completion_tokens = total_content_length // 4
```

#### 5. æ—¥å¿—è®°å½•

å½“æ£€æµ‹åˆ°æ€è€ƒå†…å®¹æ—¶ï¼Œä¼šè®°å½•åˆ°æ—¥å¿—ï¼š

```python
if accumulated_reasoning:
    logger.info(
        f"Reasoning model output detected",
        session_id=session_id,
        reasoning_length=len(accumulated_reasoning),
        content_length=len(accumulated_content)
    )
```

### æµå¼è¾“å‡ºç¤ºä¾‹

**DeepSeek-R1 è¾“å‡º**ï¼š
```
data: {"choices":[{"delta":{"role":"assistant"}}]}
data: {"choices":[{"delta":{"reasoning_content":"è®©æˆ‘åˆ†æä¸€ä¸‹è¿™ä¸ªé—®é¢˜..."}}]}
data: {"choices":[{"delta":{"reasoning_content":"é¦–å…ˆï¼Œéœ€è¦è€ƒè™‘..."}}]}
data: {"choices":[{"delta":{"reasoning_content":"ç„¶å..."}}]}
data: {"choices":[{"delta":{"content":"ç­”æ¡ˆæ˜¯"}}]}
data: {"choices":[{"delta":{"content":"42"}}]}
data: {"choices":[{"delta":{},"finish_reason":"stop"}]}
data: [DONE]
```

### æµ‹è¯•è¦†ç›–

æ–°å¢æµ‹è¯•æ–‡ä»¶ `tests/test_reasoning_models.py`ï¼š

- âœ… æµ‹è¯• `reasoning_content` å­—æ®µæ”¯æŒ
- âœ… æµ‹è¯• `thinking` å­—æ®µæ”¯æŒ
- âœ… æµ‹è¯•æ··åˆå†…å®¹ï¼ˆæ€è€ƒ + ç­”æ¡ˆï¼‰
- âœ… æµ‹è¯• SSE æ ¼å¼å…¼å®¹æ€§
- âœ… æµ‹è¯• token ç»Ÿè®¡åŒ…å«æ€è€ƒå†…å®¹
- âœ… æµ‹è¯•åªæœ‰æ€è€ƒå†…å®¹çš„æƒ…å†µ

### å®¢æˆ·ç«¯ä½¿ç”¨

å®¢æˆ·ç«¯å¯ä»¥é€‰æ‹©å¦‚ä½•å¤„ç†æ€è€ƒå†…å®¹ï¼š

**é€‰é¡¹ 1ï¼šåªæ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ**
```python
async for line in response.aiter_lines():
    if line.startswith("data: "):
        chunk = json.loads(line[6:])
        # åªå¤„ç† content å­—æ®µ
        content = chunk["choices"][0]["delta"].get("content", "")
        if content:
            print(content, end="")
```

**é€‰é¡¹ 2ï¼šåˆ†åˆ«æ˜¾ç¤ºæ€è€ƒå’Œç­”æ¡ˆ**
```python
async for line in response.aiter_lines():
    if line.startswith("data: "):
        chunk = json.loads(line[6:])
        delta = chunk["choices"][0]["delta"]
        
        # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆç°è‰²ï¼‰
        if delta.get("reasoning_content"):
            print(f"\033[90m{delta['reasoning_content']}\033[0m", end="")
        
        # æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆï¼ˆæ­£å¸¸é¢œè‰²ï¼‰
        if delta.get("content"):
            print(delta["content"], end="")
```

### æ€§èƒ½å½±å“

- âœ… **æ— é¢å¤–å¼€é”€**: åªæ˜¯ç´¯ç§¯é¢å¤–å­—æ®µï¼Œä¸å½±å“æ€§èƒ½
- âœ… **å†…å­˜å‹å¥½**: æ€è€ƒå†…å®¹ä¸å­˜å‚¨åˆ°ä¼šè¯å†å²
- âœ… **å®Œå…¨é€æ˜**: å®¢æˆ·ç«¯å¯ä»¥é€‰æ‹©æ˜¯å¦å¤„ç†æ€è€ƒå†…å®¹

### æœªæ¥æ”¹è¿›

å¯èƒ½çš„å¢å¼ºåŠŸèƒ½ï¼š

- [ ] å¯é…ç½®æ˜¯å¦å­˜å‚¨æ€è€ƒå†…å®¹åˆ°ä¼šè¯å†å²
- [ ] æ”¯æŒæ€è€ƒå†…å®¹çš„å•ç‹¬ä¸Šä¸‹æ–‡ç®¡ç†
- [ ] æä¾›æ€è€ƒå†…å®¹çš„æ‘˜è¦åŠŸèƒ½
- [ ] æ›´ç²¾ç¡®çš„æ€è€ƒå†…å®¹ token è®¡æ•°

### ç›¸å…³æ–‡ä»¶

- `src/api/endpoints.py` - æ€è€ƒå†…å®¹ç´¯ç§¯é€»è¾‘
- `tests/test_reasoning_models.py` - æ€è€ƒæ¨¡å‹æµ‹è¯•
- `docs/STREAMING.md` - æ–‡æ¡£æ›´æ–°

### æ€»ç»“

é€šè¿‡è¿™æ¬¡æ”¹è¿›ï¼Œæµå¼ä¼ è¾“åŠŸèƒ½ç°åœ¨å®Œå…¨å…¼å®¹æ€è€ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿï¼š

âœ… æ­£ç¡®å¤„ç† `reasoning_content` å’Œ `thinking` å­—æ®µ
âœ… å®Œæ•´è½¬å‘æ‰€æœ‰ delta å­—æ®µç»™å®¢æˆ·ç«¯
âœ… æ™ºèƒ½ç´¯ç§¯å’Œå­˜å‚¨å†…å®¹
âœ… å‡†ç¡®ç»Ÿè®¡åŒ…å«æ€è€ƒå†…å®¹çš„ token ä½¿ç”¨
âœ… æä¾›çµæ´»çš„å®¢æˆ·ç«¯å¤„ç†é€‰é¡¹

è¿™ä½¿å¾—ä¸­é—´ä»¶èƒ½å¤Ÿæ— ç¼æ”¯æŒæœ€æ–°çš„æ¨ç†æ¨¡å‹ï¼Œä¸ºç”¨æˆ·æä¾›æ›´å¼ºå¤§çš„ AI èƒ½åŠ›ã€‚
